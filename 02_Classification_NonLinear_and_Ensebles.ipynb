{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e268bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas e prontas para Classificação Ensemble (Pima Indians Diabetes).\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 1: Setup e Carregamento de Bibliotecas ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "import os \n",
    "\n",
    "# Bibliotecas de Pré-processamento e Métricas\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "# Modelos Non_Linear para classificação\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN para Classificação\n",
    "# Modelos de Ensemble para Classificação\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "import xgboost as xgb # Gradient Boosting (XGBoost)\n",
    "\n",
    "print(\"Bibliotecas importadas e prontas para Classificação NonLinear and Ensemble (Pima Indians Diabetes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57751c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados Pima Indians Diabetes (scaled) carregados com sucesso do diretório 'data/'.\n",
      "Shape do X_train: (537, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 2: Carregamento de Dados Pima (Arquivos Salvos no Notebook 03) ---\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# Carregamento dos conjuntos padronizados e separados \n",
    "try:\n",
    "    X_train_scaled = joblib.load(os.path.join(DATA_DIR, 'X_train_pima_scaled.joblib'))\n",
    "    X_test_scaled = joblib.load(os.path.join(DATA_DIR, 'X_test_pima_scaled.joblib'))\n",
    "    y_train = joblib.load(os.path.join(DATA_DIR, 'y_train_pima.joblib'))\n",
    "    y_test = joblib.load(os.path.join(DATA_DIR, 'y_test_pima.joblib'))\n",
    "    print(\"Dados Pima Indians Diabetes (scaled) carregados com sucesso do diretório 'data/'.\")\n",
    "except Exception as e:\n",
    "    # Se os arquivos não existirem, será necessário rodar a Célula 2 do Notebook 03\n",
    "    print(f\"ERRO: Não foi possível carregar os arquivos. Verifique se a Célula 2 do Notebook 03 foi executada. Erro: {e}\")\n",
    "\n",
    "print(f\"Shape do X_train: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1631e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo KNN Classifier otimizado e salvo em: models\\knn_classifier_pima.joblib\n",
      "\n",
      "KNN: Melhor Parâmetro encontrado: {'n_neighbors': 21, 'weights': 'distance'}\n",
      "KNN: Acurácia (Teste): 0.7706\n",
      "KNN: AUC-ROC (Teste): 0.8127\n",
      "\n",
      "Relatório de Classificação (KNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       150\n",
      "           1       0.73      0.54      0.62        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.76      0.72      0.73       231\n",
      "weighted avg       0.77      0.77      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 3: KNN Classifier ---\n",
    "\n",
    "# 1. Definir o range de hiperparâmetros (Número de Vizinhos K)\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [5, 7, 11, 15, 21], # Valores comuns de K\n",
    "    'weights': ['uniform', 'distance'] # Diferentes formas de ponderar os vizinhos\n",
    "}\n",
    "\n",
    "# 2. Configurar o GridSearchCV\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid_knn,\n",
    "    scoring='roc_auc', # Otimizamos para AUC-ROC\n",
    "    cv=5, \n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Treinar\n",
    "# O KNN é altamente sensível à escala, por isso é crucial usar os dados SCALED\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador\n",
    "best_knn_model = knn_grid.best_estimator_\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH_KNN = os.path.join(MODEL_DIR, 'knn_classifier_pima.joblib')\n",
    "joblib.dump(best_knn_model, MODEL_PATH_KNN)\n",
    "print(f\"\\nModelo KNN Classifier otimizado e salvo em: {MODEL_PATH_KNN}\")\n",
    "\n",
    "# 5. Avaliação\n",
    "y_pred_proba_knn = best_knn_model.predict_proba(X_test_scaled)[:, 1] # Probabilidade da Classe 1\n",
    "y_pred_knn = best_knn_model.predict(X_test_scaled) # Previsão da classe\n",
    "\n",
    "auc_knn = roc_auc_score(y_test, y_pred_proba_knn)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"\\nKNN: Melhor Parâmetro encontrado: {knn_grid.best_params_}\")\n",
    "print(f\"KNN: Acurácia (Teste): {accuracy_knn:.4f}\")\n",
    "print(f\"KNN: AUC-ROC (Teste): {auc_knn:.4f}\")\n",
    "print(\"\\nRelatório de Classificação (KNN):\")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo Random Forest Classifier otimizado e salvo em: models\\random_forest_classifier_pima.joblib\n",
      "\n",
      "Random Forest: Melhor Parâmetro encontrado: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Random Forest: Acurácia (Teste): 0.7359\n",
      "Random Forest: AUC-ROC (Teste): 0.8305\n",
      "\n",
      "Relatório de Classificação (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       150\n",
      "           1       0.67      0.49      0.57        81\n",
      "\n",
      "    accuracy                           0.74       231\n",
      "   macro avg       0.71      0.68      0.69       231\n",
      "weighted avg       0.73      0.74      0.72       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 4: Random Forest Classifier ---\n",
    "\n",
    "# 1. Definir o range de hiperparâmetros\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200], # Número de árvores\n",
    "    'max_depth': [5, 10],       # Profundidade máxima\n",
    "    'min_samples_split': [5, 10]\n",
    "}\n",
    "\n",
    "# 2. Configurar o GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='roc_auc', # Otimizamos para AUC-ROC\n",
    "    cv=5, \n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Treinar\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH_RF = os.path.join(MODEL_DIR, 'random_forest_classifier_pima.joblib')\n",
    "joblib.dump(best_rf_model, MODEL_PATH_RF)\n",
    "print(f\"\\nModelo Random Forest Classifier otimizado e salvo em: {MODEL_PATH_RF}\")\n",
    "\n",
    "# 5. Avaliação\n",
    "y_pred_proba_rf = best_rf_model.predict_proba(X_test_scaled)[:, 1] # Probabilidade da Classe 1\n",
    "y_pred_rf = best_rf_model.predict(X_test_scaled) # Previsão da classe\n",
    "\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest: Melhor Parâmetro encontrado: {rf_grid.best_params_}\")\n",
    "print(f\"Random Forest: Acurácia (Teste): {accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest: AUC-ROC (Teste): {auc_rf:.4f}\")\n",
    "print(\"\\nRelatório de Classificação (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf9761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo XGBoost Classifier otimizado e salvo em: models\\xgboost_classifier_pima.joblib\n",
      "\n",
      "XGBoost: Melhor Parâmetro encontrado: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
      "XGBoost: Acurácia (Teste): 0.7576\n",
      "XGBoost: AUC-ROC (Teste): 0.8416\n",
      "\n",
      "Relatório de Classificação (XGBoost):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       150\n",
      "           1       0.70      0.54      0.61        81\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.71      0.72       231\n",
      "weighted avg       0.75      0.76      0.75       231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RonaldoRosarioRamos\\miniconda3\\envs\\python_modelos_predicao_estatistica\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:24:41] WARNING: D:\\bld\\xgboost-split_1764148459144\\work\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 5: XGBoost Classifier (Gradient Boosting) ---\n",
    "\n",
    "# 1. Definir o range de hiperparâmetros\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],      \n",
    "    'max_depth': [3, 5],            \n",
    "    'learning_rate': [0.05, 0.1]     \n",
    "}\n",
    "\n",
    "# 2. Configurar o GridSearchCV\n",
    "# Usamos 'objective='binary:logistic' para problemas de classificação binária\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='roc_auc', # Otimizamos para AUC-ROC\n",
    "    cv=5, \n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Treinar\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador\n",
    "best_xgb_model = xgb_grid.best_estimator_\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH_XGB = os.path.join(MODEL_DIR, 'xgboost_classifier_pima.joblib')\n",
    "joblib.dump(best_xgb_model, MODEL_PATH_XGB)\n",
    "print(f\"\\nModelo XGBoost Classifier otimizado e salvo em: {MODEL_PATH_XGB}\")\n",
    "\n",
    "# 5. Avaliação\n",
    "y_pred_proba_xgb = best_xgb_model.predict_proba(X_test_scaled)[:, 1] # Probabilidade da Classe 1\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_scaled) # Previsão da classe\n",
    "\n",
    "auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"\\nXGBoost: Melhor Parâmetro encontrado: {xgb_grid.best_params_}\")\n",
    "print(f\"XGBoost: Acurácia (Teste): {accuracy_xgb:.4f}\")\n",
    "print(f\"XGBoost: AUC-ROC (Teste): {auc_xgb:.4f}\")\n",
    "print(\"\\nRelatório de Classificação (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_modelos_predicao_estatistica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
