{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee806c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas e prontas para Regressão Não Linear e Ensembles.\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 1: Setup e Carregamento de Bibliotecas ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Modelos para a etapa de Regressão Não Linear/Ensemble\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb # Ou lightgbm/catboost, vamos focar no XGBoost como principal\n",
    "\n",
    "# Para garantir o uso correto do RMSE em todas as versões do sklearn\n",
    "from math import sqrt \n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"Bibliotecas importadas e prontas para Regressão Não Linear e Ensembles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098b1339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório 'data' criado.\n",
      "Dados padronizados (X/y) salvos com sucesso em: data/\n",
      "\n",
      "Dados do California Housing carregados, padronizados e separados (Train/Test).\n",
      "Shape do X_train: (14448, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 2: Carregamento, Pré-processamento e Salvamento de Dados ---\n",
    "# 1. Carregamento e Definição de X e Y\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "housing_df = housing.frame\n",
    "housing_df['MEDV'] = housing_df['MedHouseVal'] # Variável Alvo (Y)\n",
    "\n",
    "X = housing_df.drop(['MEDV', 'MedHouseVal'], axis=1)\n",
    "y = housing_df['MEDV']\n",
    "\n",
    "# 2. Split (random_state=42 garante a mesma divisão sempre)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Padronização (Ajustar apenas no treino, transformar no treino e teste)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. SALVAMENTO DOS DADOS PARA REPRODUTIBILIDADE\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# 4.1. Garantir que o diretório exista\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR) \n",
    "    print(f\"Diretório '{DATA_DIR}' criado.\")\n",
    "\n",
    "# 4.2. Salvar os conjuntos de dados processados\n",
    "joblib.dump(X_train_scaled, os.path.join(DATA_DIR, 'X_train_scaled.joblib'))\n",
    "joblib.dump(X_test_scaled, os.path.join(DATA_DIR, 'X_test_scaled.joblib'))\n",
    "joblib.dump(y_train, os.path.join(DATA_DIR, 'y_train.joblib'))\n",
    "joblib.dump(y_test, os.path.join(DATA_DIR, 'y_test.joblib'))\n",
    "\n",
    "print(f\"Dados padronizados (X/y) salvos com sucesso em: {DATA_DIR}/\")\n",
    "\n",
    "# Converte para DataFrame para uso em células futuras, se necessário (mantendo as colunas)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "print(f\"\\nDados do California Housing carregados, padronizados e separados (Train/Test).\")\n",
    "print(f\"Shape do X_train: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abb408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório 'models' criado.\n",
      "Modelo KNN otimizado salvo em: models\\knn_regressor_model.joblib\n",
      "\n",
      "KNN: Melhor K encontrado: 11\n",
      "RMSE (Teste) - KNN: 0.6411\n",
      "R2 (Teste) - KNN: 0.6869\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 3: Regressão KNN (K-Nearest Neighbors) ---\n",
    "\n",
    "# Importa módulos necessários para esta célula (alguns já devem estar na Célula 1)\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import os \n",
    "import joblib \n",
    "from math import sqrt \n",
    "\n",
    "# 1. Definir o range de K's a serem testados\n",
    "param_grid = {'n_neighbors': range(1, 21)} # Testar de K=1 a K=20\n",
    "\n",
    "# 2. Configurar o GridSearchCV para encontrar o K ideal\n",
    "knn = KNeighborsRegressor()\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error', # Otimizar para minimizar o MSE\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    verbose=0,\n",
    "    n_jobs=-1 # Usar todos os núcleos\n",
    ")\n",
    "\n",
    "# 3. Treinar com os dados padronizados\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador encontrado (Organização e Reprodutibilidade)\n",
    "best_knn_model = knn_grid.best_estimator_\n",
    "\n",
    "# Lógica de Salvamento na pasta 'models/'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_FILENAME = 'knn_regressor_model.joblib'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME) # Caminho relativo\n",
    "\n",
    "# 4.1. Garantir que o diretório exista\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR) # Cria o diretório 'models/' se ele não existir\n",
    "    print(f\"Diretório '{MODEL_DIR}' criado.\")\n",
    "\n",
    "# 4.2. Salvar o modelo\n",
    "joblib.dump(best_knn_model, MODEL_PATH) \n",
    "print(f\"Modelo KNN otimizado salvo em: {MODEL_PATH}\")\n",
    "\n",
    "# 5. Avaliação do Modelo Otimizado (Usando o modelo salvo)\n",
    "y_pred_knn = best_knn_model.predict(X_test_scaled)\n",
    "rmse_knn = sqrt(mean_squared_error(y_test, y_pred_knn)) \n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"\\nKNN: Melhor K encontrado: {knn_grid.best_params_['n_neighbors']}\")\n",
    "print(f\"RMSE (Teste) - KNN: {rmse_knn:.4f}\")\n",
    "print(f\"R2 (Teste) - KNN: {r2_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3223de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Modelo Random Forest otimizado e salvo em: models\\random_forest_regressor_model.joblib\n",
      "\n",
      "Random Forest: Melhor Parâmetro encontrado: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "RMSE (Teste) - Random Forest: 0.5046\n",
      "R2 (Teste) - Random Forest: 0.8060\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 4: Regressão Random Forest (Ensemble) ---\n",
    "\n",
    "# Random Forest não necessita de Padronização, mas funciona bem com ela.\n",
    "\n",
    "# 1. Definir o range de hiperparâmetros (apenas uma pequena otimização inicial)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200], # Número de árvores\n",
    "    'max_depth': [10, 20],      # Profundidade máxima da árvore\n",
    "    'min_samples_split': [5, 10]\n",
    "}\n",
    "\n",
    "# 2. Configurar o GridSearchCV\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3, # Usamos CV=3 aqui para acelerar o processo, já que são muitos dados.\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Treinar\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH_RF = os.path.join(MODEL_DIR, 'random_forest_regressor_model.joblib')\n",
    "joblib.dump(best_rf_model, MODEL_PATH_RF)\n",
    "print(f\"\\nModelo Random Forest otimizado e salvo em: {MODEL_PATH_RF}\")\n",
    "\n",
    "# 5. Avaliação\n",
    "y_pred_rf = best_rf_model.predict(X_test_scaled)\n",
    "rmse_rf = sqrt(mean_squared_error(y_test, y_pred_rf)) \n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest: Melhor Parâmetro encontrado: {rf_grid.best_params_}\")\n",
    "print(f\"RMSE (Teste) - Random Forest: {rmse_rf:.4f}\")\n",
    "print(f\"R2 (Teste) - Random Forest: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633f64a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "Modelo XGBoost otimizado e salvo em: models\\xgboost_regressor_model.joblib\n",
      "\n",
      "XGBoost: Melhor Parâmetro encontrado: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "RMSE (Teste) - XGBoost: 0.4642\n",
      "R2 (Teste) - XGBoost: 0.8358\n"
     ]
    }
   ],
   "source": [
    "# --- CÉLULA 5: Regressão XGBoost (Gradient Boosting) ---\n",
    "\n",
    "# Reutilizamos o import do XGBoost feito na Célula 1 (se a instalação deu certo)\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "# 1. Definir o range de hiperparâmetros (apenas uma pequena otimização inicial)\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],      # Número de árvores\n",
    "    'max_depth': [5, 10],            # Profundidade máxima\n",
    "    'learning_rate': [0.05, 0.1]     # Taxa de aprendizado\n",
    "}\n",
    "\n",
    "# 2. Configurar o GridSearchCV\n",
    "# Usamos o 'gbtree' como booster padrão para regressão\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3, # Usamos CV=3 para acelerar o processo\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Treinar\n",
    "# O XGBoost pode aceitar dados padronizados (scaled) ou brutos, usaremos o scaled para consistência.\n",
    "xgb_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Salvar o melhor estimador\n",
    "best_xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_PATH_XGB = os.path.join(MODEL_DIR, 'xgboost_regressor_model.joblib')\n",
    "joblib.dump(best_xgb_model, MODEL_PATH_XGB)\n",
    "print(f\"\\nModelo XGBoost otimizado e salvo em: {MODEL_PATH_XGB}\")\n",
    "\n",
    "# 5. Avaliação\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_scaled)\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test, y_pred_xgb)) \n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"\\nXGBoost: Melhor Parâmetro encontrado: {xgb_grid.best_params_}\")\n",
    "print(f\"RMSE (Teste) - XGBoost: {rmse_xgb:.4f}\")\n",
    "print(f\"R2 (Teste) - XGBoost: {r2_xgb:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_modelos_predicao_estatistica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
